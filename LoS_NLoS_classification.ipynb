{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b980fb99",
   "metadata": {},
   "source": [
    "## Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d18ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io \n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606c205",
   "metadata": {},
   "source": [
    "## Dataset Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dca00",
   "metadata": {},
   "source": [
    "### Read in the original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c7763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_origin = torch.load('Dataset/train_dl.pt')\n",
    "valid_dl_origin = torch.load('Dataset/valid_dl.pt')\n",
    "\n",
    "train_CSI = train_dl_origin.dataset[:][0]\n",
    "train_label = train_dl_origin.dataset[:][1][:,2].type(torch.LongTensor)\n",
    "\n",
    "valid_CSI = valid_dl_origin.dataset[:][0]\n",
    "valid_label = valid_dl_origin.dataset[:][1][:,2].type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c1e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0,  ..., 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee997e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15000])\n"
     ]
    }
   ],
   "source": [
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7566af9",
   "metadata": {},
   "source": [
    "### CSI Processing: Take Modulus of complex matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91b91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CSI_modulus = torch.abs(train_CSI)\n",
    "valid_CSI_modulus = torch.abs(valid_CSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fb12a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[100.6578, 124.7878, 106.1179,  ..., 304.7704, 299.6064, 324.3594],\n",
      "          [132.8157, 106.6771,  91.2688,  ..., 269.1561, 323.5568, 299.9617],\n",
      "          [129.1395, 148.4756, 170.0735,  ..., 399.8112, 407.4420, 402.0112],\n",
      "          [ 74.0000,  71.4493,  59.3633,  ..., 134.0149, 129.6919, 124.0363]]],\n",
      "\n",
      "\n",
      "        [[[177.0198, 170.4963, 169.1065,  ...,  46.6154,  37.6431,  64.4981],\n",
      "          [143.6802, 143.0874,  88.0909,  ...,  44.0454,  22.2036,  27.6586],\n",
      "          [ 97.8008,  80.7527,  71.7008,  ...,  32.2025,  22.4722,  39.3573],\n",
      "          [ 39.8121,  45.7930,  31.6228,  ...,  16.5529,   8.0623,  25.6125]]],\n",
      "\n",
      "\n",
      "        [[[411.3940, 421.5412, 380.1276,  ..., 509.8431, 550.0582, 539.8120],\n",
      "          [366.8079, 387.3629, 353.0340,  ..., 596.1241, 619.6975, 605.5353],\n",
      "          [574.8991, 593.8560, 612.0008,  ..., 928.9521, 923.3618, 914.0552],\n",
      "          [289.8362, 287.2368, 281.0427,  ..., 354.9113, 339.0634, 333.9461]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[296.5889, 288.2672, 292.7695,  ..., 388.4289, 365.5380, 326.5287],\n",
      "          [273.3862, 265.4525, 250.9283,  ..., 330.0682, 296.2735, 331.4845],\n",
      "          [176.4795, 186.6039, 184.8161,  ..., 353.2775, 364.3350, 341.8786],\n",
      "          [ 90.3770,  88.4138, 101.9510,  ..., 153.5838, 164.6390, 143.2096]]],\n",
      "\n",
      "\n",
      "        [[[316.8359, 295.8463, 308.2872,  ..., 268.6001, 285.3086, 291.7602],\n",
      "          [240.7094, 218.8744, 216.6887,  ..., 127.0118, 118.3427, 114.0044],\n",
      "          [158.0506, 149.5761, 137.2443,  ...,  66.8506,  52.4976,  70.6045],\n",
      "          [119.0798, 103.5761, 110.1454,  ...,  85.7555,  70.0071,  75.8947]]],\n",
      "\n",
      "\n",
      "        [[[ 80.6102,  84.8528,  80.0625,  ..., 321.7111, 336.7566, 329.2567],\n",
      "          [ 55.4437,  73.0616, 118.0381,  ..., 272.0294, 295.6772, 295.6772],\n",
      "          [ 97.9898, 130.2306, 116.2755,  ..., 316.7649, 289.8034, 316.6591],\n",
      "          [104.1777, 115.9353, 104.4031,  ..., 116.6619, 135.9779, 111.5213]]]])\n"
     ]
    }
   ],
   "source": [
    "print(train_CSI_modulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631db329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15000, 1, 4, 1632])\n",
      "torch.Size([5000, 1, 4, 1632])\n"
     ]
    }
   ],
   "source": [
    "print(train_CSI_modulus.shape)\n",
    "print(valid_CSI_modulus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9fa7c",
   "metadata": {},
   "source": [
    "###  CSI Processing: Normalize to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0564be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "min_value = torch.min(train_CSI_modulus)\n",
    "max_value = torch.max(train_CSI_modulus)\n",
    "\n",
    "normalized_train_CSI_modulus = (train_CSI_modulus - min_value) / (max_value - min_value)\n",
    "normalized_valid_CSI_modulus = (valid_CSI_modulus - min_value) / (max_value - min_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3807eb1",
   "metadata": {},
   "source": [
    "### ML Classifcation w/ KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ad6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 4d to 2d\n",
    "train_data_2d = normalized_train_CSI_modulus.view(normalized_train_CSI_modulus.size(0), -1)  # Reshape to (15000, 4 * 1632)\n",
    "valid_data_2d = normalized_valid_CSI_modulus.view(normalized_valid_CSI_modulus.size(0), -1)  # Reshape to (15000, 4 * 1632)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728ce92",
   "metadata": {},
   "source": [
    "#### Below tests multiple k-values\n",
    "    Don't recommend running as it takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfe840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "sq = int(math.sqrt(15000))\n",
    "accs = {}\n",
    "for i in range(sq-30, sq+1, 10): \n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(train_data_2d, train_label)\n",
    "    print('Accuracy of KNN classifier on training set: {:.2f}'.format(knn.score(train_data_2d, train_label)))\n",
    "    accs[i] = knn.score(train_data_2d, train_label)\n",
    "print(\"Best K-Neighbors: \", max(accs, key=accs.get))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b3cda84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neighbors\\_classification.py:200: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "C:\\Users\\Joey\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.85\n",
      "Accuracy of KNN classifier on valid set: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "sq = int(math.sqrt(15000))\n",
    "knn = KNeighborsClassifier(n_neighbors = sq - 35)\n",
    "#Train the model using the training sets\n",
    "knn.fit(train_data_2d, train_label)\n",
    "#Test the model using the testing sets\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'.format(knn.score(train_data_2d, train_label)))\n",
    "print('Accuracy of KNN classifier on valid set: {:.2f}'.format(knn.score(valid_data_2d, valid_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f050050",
   "metadata": {},
   "source": [
    "### ANN approach (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8634c1b",
   "metadata": {},
   "source": [
    "- Instantiate a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692a9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=6528, out_features=300, bias=True)\n",
      "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (layer3): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (layer4): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(4*1632, 300)  # input layer (4*1632 nodes) -> hidden layer (100 nodes)\n",
    "        self.layer2 = nn.Linear(300, 300)  # hidden layer (100 nodes) -> hidden layer (100 nodes)\n",
    "        self.layer3 = nn.Linear(300, 100)  # hidden layer (100 nodes) -> hidden layer (100 nodes)\n",
    "        self.layer4 = nn.Linear(100, 1)  # hidden layer (100 nodes) -> output layer (1 nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer4(F.relu(self.layer3(F.relu(self.layer2(F.relu(self.layer1(x)))))))\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82a460",
   "metadata": {},
   "source": [
    "- Add a loss function and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "064e0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # define the loss function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.003, momentum=0.9)  # define the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a740b5",
   "metadata": {},
   "source": [
    "- Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f694136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Temp\\ipykernel_2972\\4240162415.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data_2d = torch.tensor(train_data_2d, dtype=torch.float32)\n",
      "C:\\Users\\Joey\\AppData\\Local\\Temp\\ipykernel_2972\\4240162415.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_label = torch.tensor(train_label, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.6729807257652283\n",
      "Epoch [2/300], Loss: 0.6725283861160278\n",
      "Epoch [3/300], Loss: 0.67167067527771\n",
      "Epoch [4/300], Loss: 0.6704527139663696\n",
      "Epoch [5/300], Loss: 0.6689159274101257\n",
      "Epoch [6/300], Loss: 0.6671010255813599\n",
      "Epoch [7/300], Loss: 0.6650528907775879\n",
      "Epoch [8/300], Loss: 0.6628193855285645\n",
      "Epoch [9/300], Loss: 0.6604406833648682\n",
      "Epoch [10/300], Loss: 0.6579378247261047\n",
      "Epoch [11/300], Loss: 0.6553328633308411\n",
      "Epoch [12/300], Loss: 0.6526481509208679\n",
      "Epoch [13/300], Loss: 0.6498970985412598\n",
      "Epoch [14/300], Loss: 0.6470907330513\n",
      "Epoch [15/300], Loss: 0.644235372543335\n",
      "Epoch [16/300], Loss: 0.6413372159004211\n",
      "Epoch [17/300], Loss: 0.6384027600288391\n",
      "Epoch [18/300], Loss: 0.6354385614395142\n",
      "Epoch [19/300], Loss: 0.6324502825737\n",
      "Epoch [20/300], Loss: 0.6294431686401367\n",
      "Epoch [21/300], Loss: 0.6264225244522095\n",
      "Epoch [22/300], Loss: 0.6233915090560913\n",
      "Epoch [23/300], Loss: 0.6203547120094299\n",
      "Epoch [24/300], Loss: 0.6173157691955566\n",
      "Epoch [25/300], Loss: 0.6142776012420654\n",
      "Epoch [26/300], Loss: 0.6112430095672607\n",
      "Epoch [27/300], Loss: 0.608213484287262\n",
      "Epoch [28/300], Loss: 0.6051909327507019\n",
      "Epoch [29/300], Loss: 0.6021759510040283\n",
      "Epoch [30/300], Loss: 0.5991678833961487\n",
      "Epoch [31/300], Loss: 0.5961665511131287\n",
      "Epoch [32/300], Loss: 0.5931714177131653\n",
      "Epoch [33/300], Loss: 0.590181827545166\n",
      "Epoch [34/300], Loss: 0.5871983766555786\n",
      "Epoch [35/300], Loss: 0.584221601486206\n",
      "Epoch [36/300], Loss: 0.5812519788742065\n",
      "Epoch [37/300], Loss: 0.5782900452613831\n",
      "Epoch [38/300], Loss: 0.5753362774848938\n",
      "Epoch [39/300], Loss: 0.572390615940094\n",
      "Epoch [40/300], Loss: 0.5694526433944702\n",
      "Epoch [41/300], Loss: 0.5665225386619568\n",
      "Epoch [42/300], Loss: 0.5635998845100403\n",
      "Epoch [43/300], Loss: 0.560684323310852\n",
      "Epoch [44/300], Loss: 0.5577763915061951\n",
      "Epoch [45/300], Loss: 0.5548762083053589\n",
      "Epoch [46/300], Loss: 0.5519841313362122\n",
      "Epoch [47/300], Loss: 0.5490996837615967\n",
      "Epoch [48/300], Loss: 0.5462220311164856\n",
      "Epoch [49/300], Loss: 0.5433509945869446\n",
      "Epoch [50/300], Loss: 0.5404865741729736\n",
      "Epoch [51/300], Loss: 0.5376286506652832\n",
      "Epoch [52/300], Loss: 0.5347769856452942\n",
      "Epoch [53/300], Loss: 0.5319314002990723\n",
      "Epoch [54/300], Loss: 0.5290921926498413\n",
      "Epoch [55/300], Loss: 0.5262593626976013\n",
      "Epoch [56/300], Loss: 0.5234331488609314\n",
      "Epoch [57/300], Loss: 0.5206140875816345\n",
      "Epoch [58/300], Loss: 0.517802357673645\n",
      "Epoch [59/300], Loss: 0.5149983763694763\n",
      "Epoch [60/300], Loss: 0.5122029781341553\n",
      "Epoch [61/300], Loss: 0.5094172358512878\n",
      "Epoch [62/300], Loss: 0.50664222240448\n",
      "Epoch [63/300], Loss: 0.5038793683052063\n",
      "Epoch [64/300], Loss: 0.5011301636695862\n",
      "Epoch [65/300], Loss: 0.49839621782302856\n",
      "Epoch [66/300], Loss: 0.49567919969558716\n",
      "Epoch [67/300], Loss: 0.49298095703125\n",
      "Epoch [68/300], Loss: 0.4903039038181305\n",
      "Epoch [69/300], Loss: 0.4876503646373749\n",
      "Epoch [70/300], Loss: 0.4850231111049652\n",
      "Epoch [71/300], Loss: 0.48242509365081787\n",
      "Epoch [72/300], Loss: 0.479859322309494\n",
      "Epoch [73/300], Loss: 0.4773290753364563\n",
      "Epoch [74/300], Loss: 0.47483813762664795\n",
      "Epoch [75/300], Loss: 0.47238990664482117\n",
      "Epoch [76/300], Loss: 0.46998798847198486\n",
      "Epoch [77/300], Loss: 0.4676360785961151\n",
      "Epoch [78/300], Loss: 0.465337872505188\n",
      "Epoch [79/300], Loss: 0.4630967378616333\n",
      "Epoch [80/300], Loss: 0.46091607213020325\n",
      "Epoch [81/300], Loss: 0.45879921317100525\n",
      "Epoch [82/300], Loss: 0.45674923062324524\n",
      "Epoch [83/300], Loss: 0.4547690451145172\n",
      "Epoch [84/300], Loss: 0.45286139845848083\n",
      "Epoch [85/300], Loss: 0.45102840662002563\n",
      "Epoch [86/300], Loss: 0.4492722451686859\n",
      "Epoch [87/300], Loss: 0.44759446382522583\n",
      "Epoch [88/300], Loss: 0.4459964334964752\n",
      "Epoch [89/300], Loss: 0.4444790482521057\n",
      "Epoch [90/300], Loss: 0.44304290413856506\n",
      "Epoch [91/300], Loss: 0.4416881799697876\n",
      "Epoch [92/300], Loss: 0.44041457772254944\n",
      "Epoch [93/300], Loss: 0.4392213225364685\n",
      "Epoch [94/300], Loss: 0.4381074011325836\n",
      "Epoch [95/300], Loss: 0.4370712339878082\n",
      "Epoch [96/300], Loss: 0.4361110031604767\n",
      "Epoch [97/300], Loss: 0.43522462248802185\n",
      "Epoch [98/300], Loss: 0.4344094395637512\n",
      "Epoch [99/300], Loss: 0.4336627423763275\n",
      "Epoch [100/300], Loss: 0.43298134207725525\n",
      "Epoch [101/300], Loss: 0.4323621094226837\n",
      "Epoch [102/300], Loss: 0.4318016469478607\n",
      "Epoch [103/300], Loss: 0.43129655718803406\n",
      "Epoch [104/300], Loss: 0.4308430552482605\n",
      "Epoch [105/300], Loss: 0.4304378926753998\n",
      "Epoch [106/300], Loss: 0.430077463388443\n",
      "Epoch [107/300], Loss: 0.429758220911026\n",
      "Epoch [108/300], Loss: 0.42947691679000854\n",
      "Epoch [109/300], Loss: 0.4292302131652832\n",
      "Epoch [110/300], Loss: 0.42901474237442017\n",
      "Epoch [111/300], Loss: 0.4288276433944702\n",
      "Epoch [112/300], Loss: 0.4286658763885498\n",
      "Epoch [113/300], Loss: 0.42852672934532166\n",
      "Epoch [114/300], Loss: 0.4284074604511261\n",
      "Epoch [115/300], Loss: 0.4283056855201721\n",
      "Epoch [116/300], Loss: 0.4282190799713135\n",
      "Epoch [117/300], Loss: 0.4281456768512726\n",
      "Epoch [118/300], Loss: 0.4280834496021271\n",
      "Epoch [119/300], Loss: 0.428030788898468\n",
      "Epoch [120/300], Loss: 0.4279859662055969\n",
      "Epoch [121/300], Loss: 0.4279477596282959\n",
      "Epoch [122/300], Loss: 0.4279147982597351\n",
      "Epoch [123/300], Loss: 0.42788615822792053\n",
      "Epoch [124/300], Loss: 0.427860826253891\n",
      "Epoch [125/300], Loss: 0.42783811688423157\n",
      "Epoch [126/300], Loss: 0.4278171956539154\n",
      "Epoch [127/300], Loss: 0.42779767513275146\n",
      "Epoch [128/300], Loss: 0.4277789890766144\n",
      "Epoch [129/300], Loss: 0.4277607798576355\n",
      "Epoch [130/300], Loss: 0.42774271965026855\n",
      "Epoch [131/300], Loss: 0.427724689245224\n",
      "Epoch [132/300], Loss: 0.42770639061927795\n",
      "Epoch [133/300], Loss: 0.42768779397010803\n",
      "Epoch [134/300], Loss: 0.4276687800884247\n",
      "Epoch [135/300], Loss: 0.42764928936958313\n",
      "Epoch [136/300], Loss: 0.42762935161590576\n",
      "Epoch [137/300], Loss: 0.4276089370250702\n",
      "Epoch [138/300], Loss: 0.4275880753993988\n",
      "Epoch [139/300], Loss: 0.4275668263435364\n",
      "Epoch [140/300], Loss: 0.42754530906677246\n",
      "Epoch [141/300], Loss: 0.4275234341621399\n",
      "Epoch [142/300], Loss: 0.42750129103660583\n",
      "Epoch [143/300], Loss: 0.42747896909713745\n",
      "Epoch [144/300], Loss: 0.4274565577507019\n",
      "Epoch [145/300], Loss: 0.4274340271949768\n",
      "Epoch [146/300], Loss: 0.4274114668369293\n",
      "Epoch [147/300], Loss: 0.4273889362812042\n",
      "Epoch [148/300], Loss: 0.4273664057254791\n",
      "Epoch [149/300], Loss: 0.4273439645767212\n",
      "Epoch [150/300], Loss: 0.4273216426372528\n",
      "Epoch [151/300], Loss: 0.42729946970939636\n",
      "Epoch [152/300], Loss: 0.42727744579315186\n",
      "Epoch [153/300], Loss: 0.42725563049316406\n",
      "Epoch [154/300], Loss: 0.4272339940071106\n",
      "Epoch [155/300], Loss: 0.42721250653266907\n",
      "Epoch [156/300], Loss: 0.42719128727912903\n",
      "Epoch [157/300], Loss: 0.4271702170372009\n",
      "Epoch [158/300], Loss: 0.42714935541152954\n",
      "Epoch [159/300], Loss: 0.42712870240211487\n",
      "Epoch [160/300], Loss: 0.42710819840431213\n",
      "Epoch [161/300], Loss: 0.4270879924297333\n",
      "Epoch [162/300], Loss: 0.4270678758621216\n",
      "Epoch [163/300], Loss: 0.4270479679107666\n",
      "Epoch [164/300], Loss: 0.4270282983779907\n",
      "Epoch [165/300], Loss: 0.427008718252182\n",
      "Epoch [166/300], Loss: 0.42698928713798523\n",
      "Epoch [167/300], Loss: 0.42697012424468994\n",
      "Epoch [168/300], Loss: 0.42695099115371704\n",
      "Epoch [169/300], Loss: 0.42693206667900085\n",
      "Epoch [170/300], Loss: 0.42691320180892944\n",
      "Epoch [171/300], Loss: 0.42689454555511475\n",
      "Epoch [172/300], Loss: 0.4268759787082672\n",
      "Epoch [173/300], Loss: 0.42685750126838684\n",
      "Epoch [174/300], Loss: 0.4268391728401184\n",
      "Epoch [175/300], Loss: 0.42682084441185\n",
      "Epoch [176/300], Loss: 0.4268026649951935\n",
      "Epoch [177/300], Loss: 0.42678454518318176\n",
      "Epoch [178/300], Loss: 0.4267665445804596\n",
      "Epoch [179/300], Loss: 0.4267486035823822\n",
      "Epoch [180/300], Loss: 0.426730751991272\n",
      "Epoch [181/300], Loss: 0.4267129898071289\n",
      "Epoch [182/300], Loss: 0.426695317029953\n",
      "Epoch [183/300], Loss: 0.4266776442527771\n",
      "Epoch [184/300], Loss: 0.42666009068489075\n",
      "Epoch [185/300], Loss: 0.4266425669193268\n",
      "Epoch [186/300], Loss: 0.42662513256073\n",
      "Epoch [187/300], Loss: 0.42660778760910034\n",
      "Epoch [188/300], Loss: 0.4265904426574707\n",
      "Epoch [189/300], Loss: 0.4265732169151306\n",
      "Epoch [190/300], Loss: 0.4265560507774353\n",
      "Epoch [191/300], Loss: 0.4265389144420624\n",
      "Epoch [192/300], Loss: 0.42652183771133423\n",
      "Epoch [193/300], Loss: 0.4265049397945404\n",
      "Epoch [194/300], Loss: 0.4264880120754242\n",
      "Epoch [195/300], Loss: 0.4264712929725647\n",
      "Epoch [196/300], Loss: 0.4264545440673828\n",
      "Epoch [197/300], Loss: 0.42643794417381287\n",
      "Epoch [198/300], Loss: 0.42642146348953247\n",
      "Epoch [199/300], Loss: 0.4264049530029297\n",
      "Epoch [200/300], Loss: 0.42638856172561646\n",
      "Epoch [201/300], Loss: 0.4263722002506256\n",
      "Epoch [202/300], Loss: 0.42635592818260193\n",
      "Epoch [203/300], Loss: 0.426339715719223\n",
      "Epoch [204/300], Loss: 0.42632347345352173\n",
      "Epoch [205/300], Loss: 0.4263073801994324\n",
      "Epoch [206/300], Loss: 0.4262913167476654\n",
      "Epoch [207/300], Loss: 0.42627522349357605\n",
      "Epoch [208/300], Loss: 0.42625921964645386\n",
      "Epoch [209/300], Loss: 0.42624330520629883\n",
      "Epoch [210/300], Loss: 0.42622730135917664\n",
      "Epoch [211/300], Loss: 0.42621147632598877\n",
      "Epoch [212/300], Loss: 0.4261957108974457\n",
      "Epoch [213/300], Loss: 0.4261799156665802\n",
      "Epoch [214/300], Loss: 0.4261641800403595\n",
      "Epoch [215/300], Loss: 0.42614850401878357\n",
      "Epoch [216/300], Loss: 0.4261328876018524\n",
      "Epoch [217/300], Loss: 0.42611727118492126\n",
      "Epoch [218/300], Loss: 0.4261017143726349\n",
      "Epoch [219/300], Loss: 0.42608627676963806\n",
      "Epoch [220/300], Loss: 0.42607080936431885\n",
      "Epoch [221/300], Loss: 0.4260554015636444\n",
      "Epoch [222/300], Loss: 0.4260399639606476\n",
      "Epoch [223/300], Loss: 0.4260246455669403\n",
      "Epoch [224/300], Loss: 0.4260093867778778\n",
      "Epoch [225/300], Loss: 0.42599406838417053\n",
      "Epoch [226/300], Loss: 0.4259788691997528\n",
      "Epoch [227/300], Loss: 0.42596372961997986\n",
      "Epoch [228/300], Loss: 0.4259485900402069\n",
      "Epoch [229/300], Loss: 0.42593345046043396\n",
      "Epoch [230/300], Loss: 0.4259184002876282\n",
      "Epoch [231/300], Loss: 0.4259033501148224\n",
      "Epoch [232/300], Loss: 0.4258883595466614\n",
      "Epoch [233/300], Loss: 0.42587336897850037\n",
      "Epoch [234/300], Loss: 0.4258584678173065\n",
      "Epoch [235/300], Loss: 0.42584359645843506\n",
      "Epoch [236/300], Loss: 0.4258287250995636\n",
      "Epoch [237/300], Loss: 0.4258139431476593\n",
      "Epoch [238/300], Loss: 0.4257992208003998\n",
      "Epoch [239/300], Loss: 0.42578449845314026\n",
      "Epoch [240/300], Loss: 0.4257698655128479\n",
      "Epoch [241/300], Loss: 0.42575526237487793\n",
      "Epoch [242/300], Loss: 0.42574068903923035\n",
      "Epoch [243/300], Loss: 0.42572611570358276\n",
      "Epoch [244/300], Loss: 0.42571160197257996\n",
      "Epoch [245/300], Loss: 0.42569708824157715\n",
      "Epoch [246/300], Loss: 0.4256826937198639\n",
      "Epoch [247/300], Loss: 0.42566829919815063\n",
      "Epoch [248/300], Loss: 0.4256539046764374\n",
      "Epoch [249/300], Loss: 0.4256395697593689\n",
      "Epoch [250/300], Loss: 0.4256252944469452\n",
      "Epoch [251/300], Loss: 0.4256109893321991\n",
      "Epoch [252/300], Loss: 0.42559677362442017\n",
      "Epoch [253/300], Loss: 0.42558255791664124\n",
      "Epoch [254/300], Loss: 0.4255683720111847\n",
      "Epoch [255/300], Loss: 0.4255542755126953\n",
      "Epoch [256/300], Loss: 0.42554008960723877\n",
      "Epoch [257/300], Loss: 0.42552605271339417\n",
      "Epoch [258/300], Loss: 0.4255119562149048\n",
      "Epoch [259/300], Loss: 0.4254979193210602\n",
      "Epoch [260/300], Loss: 0.4254838526248932\n",
      "Epoch [261/300], Loss: 0.42546984553337097\n",
      "Epoch [262/300], Loss: 0.42545586824417114\n",
      "Epoch [263/300], Loss: 0.4254418611526489\n",
      "Epoch [264/300], Loss: 0.42542797327041626\n",
      "Epoch [265/300], Loss: 0.4254140555858612\n",
      "Epoch [266/300], Loss: 0.42540016770362854\n",
      "Epoch [267/300], Loss: 0.4253862500190735\n",
      "Epoch [268/300], Loss: 0.425372451543808\n",
      "Epoch [269/300], Loss: 0.4253586530685425\n",
      "Epoch [270/300], Loss: 0.425344854593277\n",
      "Epoch [271/300], Loss: 0.42533114552497864\n",
      "Epoch [272/300], Loss: 0.4253173768520355\n",
      "Epoch [273/300], Loss: 0.4253036379814148\n",
      "Epoch [274/300], Loss: 0.42528992891311646\n",
      "Epoch [275/300], Loss: 0.42527616024017334\n",
      "Epoch [276/300], Loss: 0.4252623915672302\n",
      "Epoch [277/300], Loss: 0.4252486228942871\n",
      "Epoch [278/300], Loss: 0.42523491382598877\n",
      "Epoch [279/300], Loss: 0.42522120475769043\n",
      "Epoch [280/300], Loss: 0.4252074360847473\n",
      "Epoch [281/300], Loss: 0.42519375681877136\n",
      "Epoch [282/300], Loss: 0.42518001794815063\n",
      "Epoch [283/300], Loss: 0.4251662790775299\n",
      "Epoch [284/300], Loss: 0.4251525104045868\n",
      "Epoch [285/300], Loss: 0.42513877153396606\n",
      "Epoch [286/300], Loss: 0.42512497305870056\n",
      "Epoch [287/300], Loss: 0.42511114478111267\n",
      "Epoch [288/300], Loss: 0.4250973165035248\n",
      "Epoch [289/300], Loss: 0.4250834584236145\n",
      "Epoch [290/300], Loss: 0.42506951093673706\n",
      "Epoch [291/300], Loss: 0.42505550384521484\n",
      "Epoch [292/300], Loss: 0.42504143714904785\n",
      "Epoch [293/300], Loss: 0.4250273108482361\n",
      "Epoch [294/300], Loss: 0.42501315474510193\n",
      "Epoch [295/300], Loss: 0.4249989986419678\n",
      "Epoch [296/300], Loss: 0.42498481273651123\n",
      "Epoch [297/300], Loss: 0.4249706268310547\n",
      "Epoch [298/300], Loss: 0.42495644092559814\n",
      "Epoch [299/300], Loss: 0.4249422550201416\n",
      "Epoch [300/300], Loss: 0.42492806911468506\n"
     ]
    }
   ],
   "source": [
    "train_data_2d = torch.tensor(train_data_2d, dtype=torch.float32)\n",
    "train_label = torch.tensor(train_label, dtype=torch.float32)\n",
    "train_label = train_label.view(-1, 1)\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = net(train_data_2d)\n",
    "    loss = criterion(output, train_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4417d900",
   "metadata": {},
   "source": [
    "- Test the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64a1bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.28%\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(model, test_data, test_labels):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(test_data)\n",
    "\n",
    "    rounded_predictions = torch.round(torch.sigmoid(test_predictions))\n",
    "    accuracy = accuracy_score(test_labels, rounded_predictions)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test_accuracy(net, valid_data_2d, valid_label)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e64ffd",
   "metadata": {},
   "source": [
    "### Using Convultional Layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
